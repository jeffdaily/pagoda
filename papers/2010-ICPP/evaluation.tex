\section{Evaluation}
\label{section:evaluation}

All tests were performed on the Franklin Cray-XT4 supercomputer\cite{franklin}
located at NERSC\cite{NERSC}.  The Franklin supercomputer features 9,572
Opteron 2.3 gigahertz quad core processors with 2 gigabytes of memory per
core.  Its Lustre parallel file system supports a theoretical peak of 16
gigabytes/second to each of two /scratch file systems.

In order to evaluate the scalability of our subsetter we performed strong
scaling tests. Data from the GCRM is not yet available, so we used data from a
GCRM precursor that runs Jablonowski's test case\cite{JAB}. This test case
uses only 25 vertical levels, instead of the 100 or so expected for the GCRM,
so data set sizes are about a quarter the size of comparable GCRM data. These
data sets are still on the order Gigabytes to 10s of Gigabytes per variable.

The performance data collected was generated by custom wrappers to the
functions we developed.  The IO wrappers perform barriers before and after
each collective IO operation is called with the start and end timestamps
collected immediately after each barrier using \verb=clock_gettime()= or
\verb=gettimeofday()= depending on platform availability.  Before program
termination, the number of bytes written and read and the total time spent
performing IO is collected on the zeroth process and displayed in terms of
Gigabytes/second.  The profiling information collects the number of times each
function is called and how much time was spent in each function.  It is only
reported for the zeroth process and is meant as a general measure of
performance.  The IO and profile data were collected on separate runs so that
the collection of the former (with additional barriers) would not affect the
latter.  This test was run over 24 timesteps of an edge variable at a 4
kilometer resolution ($R=11$) specifying a subset region corresponding to the
Madden-Julien Oscillation\cite{MJO} (20N,-20S,160E,90E).  The MJO region is
roughly 6.5\% of the global data.  One timestep of the edge variable is
12.1875 GB.  The number of processors was doubled for each run, starting from
64 and going up through 2048 processors.

Two versions of the subsetter were originally tested, one to test the
subsetter as a whole and the the other to test the algorithms by turning off
nearly all IO.  (Reading of the grid topology was still required resulting in
an insubstantial amount of IO.)  Turning off the IO is important in order to
evaluate the scalability of our packing and reindexing algorithms.  Since our
current software does not involve any algebraic operations, this case is also
an accurate representation of communication.

After the initial tests were performed, it was noted that the majority of data
being read was discarded.  An optimized version of the subsetter was developed
which determines ahead of time which processes will not participate in the
subset operation.  This allowed for the nonparticipating processes to specify
empty regions in the collective read operation, reducing the total amount of
data read prior to the subset.  It was important to show test results for both
the original and optimized versions of the subsetter in order to illustrate
the need for efficient IO algorithms as well as to accurately capture the IO
requirements when reading the entire horizontal region of data.

Fig. \ref{fig:strong} shows the timing results of the tests.  Three versions
of the code were run.  "Subsetter" represents the original program,
"Optimized" represents our optimized read, and "Algorithms" represents
executing with nearly all IO turned off.  Comparing the original subsetter to
the algorithms case, it is clear that IO represents a significant portion of
the execution time.  As the number of cores increases, the optimized case more
closely resembles the algorithms alone.  The benefit of additional processors
appears to have reached a plateau near 1024 processors for all cases, however
this may be due to the size of the problem since run times at this scale are
only a few minutes in length.

\begin{figure}[!t]
\center
\resizebox{3.5in}{!}{
\input{plots/strong_hist}
}
\caption{Strong Scaling Test.  First, the subsetter was run as a whole
program ("Subsetter").  Second, the subsetter was optimized to read less data
in the case of a subset ("Optimized").  Lastly, nearly all IO was stripped
from the program to capture the performance of the packing routines
("Algorithms").  At smaller numbers of cores, there is a greater difference
between time spent in IO and time spent everywhere else.}
\label{fig:strong}
\end{figure}

Fig. \ref{fig:strong_io} shows the IO bandwidth results of the test.  This
test captures the scalability of the IO system which correlates with the
scalability portrayed in Fig. \ref{fig:strong}.  Although in the unoptimized
case the read bandwidth may have benefited from additional cores beyond 2048,
the write bandwidth appears to have reached a plateau near 1024 processors.
The hardware is likely reaching its saturation point if not already there.
The optimized bandwidths matched those of the original subsetter given a small
amount of variability.  The notable exception is that of the optimized read
bandwidth which leveled off near 512 cores. The fact that the optimized read
tracks the unoptimized read bandwidth up to 512 processors, even though the
number of cores that are nominally reading data is smaller, reflects
optimizations in both the Parallel NetCDF and the MPI-IO libraries that are
redistributing the reads to more processors.

\begin{figure}[!t]
\center
\resizebox{3.5in}{!}{
\input{plots/strong_io_hist}
}
\caption{Strong Scaling Test for IO Bandwidth.  The Read and Write bandwidths
are compared as the number of cores are increased.  The subsetter and its
optimized versions are both shown.  In general, all bandwidths increased with
respect to the number of cores.  The optimized bandwidths matched those of the
original subsetter given a small amount of variability.  The notable exception
is that of the optimized read bandwidth which leveled off near 512 cores.}
\label{fig:strong_io}
\end{figure}

Table \ref{tab:strong_prof} represents a profile for process zero for the 2048
process run.  The function names are sufficiently self-documenting for those
familiar with C++ syntax.  The zeroth process represents a worst-case scenario
since the default distribution of data using GA will always put data on at
least the zeroth process implying process zero should have at least as much
work to perform as any other processor.  The profile for the 2048 run was
selected arbitrarily, however the profiles for the other strong scaling runs
(not shown) demonstrated a similar trend.  The functions which did not
contribute a significant amount of time to the program were removed and the
table is sorted by total time spent in the function.  The number of calls to
the read and write routines were small.  (Not shown in Table
\ref{tab:strong_prof} is the number of calls to the function comparing each
latitude/longitude pair to the desired subset region which accounts for over
60\% of the function calls.)  However, the amount of time spent in the read
and write routines was significant.  In all cases, the amount of time spent in
IO was at least 75\% of the program's execution.  IO is clearly the dominant
factor in running our code.

\begin{table*}[!t]
\center
\caption{Partial Profile for Process 0 at 2048 Cores - MJO Region}
\label{tab:strong_prof}
\begin{tabular}{lrrrrrr}
Name&Calls&Percent Calls&Time (seconds)&Percent Time&Time/call (seconds)\\
NetcdfVariable::read()                    &39&0.1&156.1&93.0&4.00\\
NetcdfFileWriter::write(int,int,int)      &34&0.1&  5.9& 3.5&0.17\\
VariableDecorator::read()                 & 5&0.0&  1.7& 1.0&0.34\\
NetcdfDataset::NetcdfDataset(string)      & 5&0.0&  1.2& 0.7&0.24\\
Dataset::adjust\_masks(LatLonBox)         & 1&0.0&  1.0& 0.6&1.05\\
ConnectivityVariable::reindex()           & 3&0.0&  0.7& 0.4&0.25\\
NetcdfFileWriter::NetcdfFileWriter(string)& 1&0.0&  0.2& 0.2&0.29\\
\end{tabular}
\end{table*}

Table \ref{tab:strong_prof_opt} represents a profile for process zero for the
2048 process run using the optimized subsetter.  The notable differences
between Table \ref{tab:strong_prof_opt} and Table \ref{tab:strong_prof} are
the total time and percentage of time spent in each function.  Although the
optimized subsetter spends substantially less time reading, the majority of
time is still spent in IO.  Although still dominant, IO is a less significant
contributor to the total execution time.

\begin{table*}[!t]
\center
\caption{Partial Profile for Process 0 at 2048 Cores - MJO Region - Optimized}
\label{tab:strong_prof_opt}
\begin{tabular}{lrrrrrr}
Name&Calls&Percent Calls&Time (seconds)&Percent Time&Time/call (seconds)\\
NetcdfVariable::read()                    & 39&0.1&22.8&65.4&0.58\\
NetcdfFileWriter::write(int,int,int)      & 34&0.1& 6.5&18.7&0.19\\
NetcdfDataset::NetcdfDataset(string)      &  5&0.0& 1.2& 3.4&0.24\\
VariableDecorator::read()                 &  5&0.0& 1.1& 3.0&0.21\\
Dataset::adjust\_masks(LatLonBox)         &  1&0.0& 0.8& 2.4&0.82\\
ConnectivityVariable::reindex()           &  3&0.0& 0.8& 2.2&0.26\\
NetcdfFileWriter::NetcdfFileWriter(string)&  1&0.0& 0.4& 1.1&0.37\\
\end{tabular}
\end{table*}

\subsection{Discussion}

Running our subsetter at a scale of over 2000 cores is initially promising.
Fig. \ref{fig:strong} demonstrates that the algorithms we developed scale to
at least 1024 processors.  The performance of the original subsetter can be
considered as the case where operations on the entire globe are performed. For
these operations, the subsetter would be reading all of the available data.
These global subsets will stress the IO system the greatest.  The optimized
subsetter will not help in cases of operations on the entire globe.  The
significant improvement afforded by the optimized subsetter is likely due to
both the data layout using a space-filling curve as explained in Section
\ref{subsection:grid} and optimization of collective read operations in both
the Parallel NetCDF and MPI-IO libraries.  The space-filling curve places
logically adjacent cells near each other in memory allowing for the maximum
number of nonparticipating processes during the read operation.

The profile of the global reads in Table \ref{tab:strong_prof} suggests that
IO accounts for nearly 95\% of program execution at 2048 cores.  This is not a
stretch of the imagination since our software reads, subsets, and then writes.
Even if our code performed a significant calculation after each read of the
edge variable, the profile would likely remain IO bound.  At fewer numbers of
cores the percentage of time spent in IO ranged from 75\% to the 95\% shown in
Table \ref{tab:strong_prof}.  Even in the optimized case, the profile remains
IO bound.

The reason for the disproportionately faster write bandwidth versus read
bandwith seen in Fig. \ref{fig:strong_io} is probably due to caching by the
Lustre parallel filesystem.  The relatively small amount of data being written
is most likely being copied to an internal buffer allowing the functions to
return quickly.
