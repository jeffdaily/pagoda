\section{Evaluation}
\label{section:evaluation}

In order to evaluate the scalability of our subsetter we performed strong
scaling tests.  All tests were performed on the franklin Cray-XT4
supercomputer\cite{franklin} located at NERSC\cite{NERSC}.  The performance
data collected was generated by custom wrappers to the functions we developed.
The IO wrappers perform barriers before and after each collective IO operation
is called with the start and end timestamps collected immediately after each
barrier using \verb=clock_gettime()= or \verb=gettimeofday()= depending on
platform availability.  Before program termination, the number of bytes
written and read and the total time spent performing IO is collected on the
zeroth process and displayed in terms of GB/s.  The profiling information
collects the number of times each function is called and how much time was
spent in each function.  It is only reported for the zeroth process and is
meant only as a general measure of performance.  The IO and function profile
data were collected on separate runs so that the collection of one would not
affect the other.

\subsection{Strong Scaling}

\begin{figure}[!t]
\center
\resizebox{3.5in}{!}{
\input{plots/strong}
}
\caption{Strong Scaling Test}
\label{fig:strong}
\end{figure}

\begin{figure}[!t]
\center
\resizebox{3.5in}{!}{
\input{plots/strong_io}
}
\caption{Strong Scaling Test - IO}
\label{fig:strong_io}
\end{figure}

\begin{table*}[!t]
\center
\caption{Strong Scaling Profile for Process 0}
\label{tab:strong_prof}
\begin{tabular}{lllllll}
Name&Calls&Prcnt&Times&Prcnt&T/call&Prcnt\\
NetcdfVariable::read()&39&0.1&47710000&0.79&1223333.3&79.5\\
NetcdfFileWriter::write(int,int,int)&34&0.1&8460000&0.14&248823.5&14.1\\
ConnectivityVariable::reindex()&3&0&1480000&0.02&493333.3&2.5\\
VariableDecorator::read()&5&0&780000&0.01&156000&1.3\\
NetcdfDataset::NetcdfDataset(string)&5&0&670000&0.01&134000&1.1\\
Dataset::adjust\_masks(LatLonBox)&1&0&600000&0.01&600000&1\\
%NetcdfFileWriter::NetcdfFileWriter(string)&1&0&180000&0&180000&0.3\\
%AbstractVariable::get\_handle()&165&0.3&140000&0&848.5&0.2\\
%pack(int,int,int*,int*)&31&0.1&70000&0&2258.1&0.1\\
%NetcdfFileWriter::maybe\_enddef()&68&0.1&60000&0&882.4&0.1\\
%Dataset::create\_masks()&1&0&50000&0&50000&0.1\\
%LatLonBox::contains(double,double)&40961&78&40000&0&1&0.1\\
%AbstractVariable::release\_handle()&50&0.1&20000&0&400&0\\
%AggregationVariable::release\_handle()&2&0&10000&0&5000&0\\
\end{tabular}
\end{table*}

This test was run over 24 timesteps of an edge variable at a 4Km resolution
(R=11) specifying a subset region of the Madden-Julien Oscillation\cite{MJO}
(20N,-20S,160E,90E).  One timestep of the edge variable is 12.1875 GB.  Figure
\ref{fig:strong} shows the timing results of the test while Figure
\ref{fig:strong_io} shows the IO bandwidth.  The number of processors was
doubled each run starting from 64.

Table \ref{tab:strong_prof} is a table of a profile for process zero for the
1024 process run.  The function names are sufficiently self-documenting for
those familiar with C++ syntax.  The zeroth process represents a worst-case
scenario since the default distribution of data using GA will always put data
on at least the zeroth process implying process zero should have at least as
much work to perform as any other processor.  The profile for the 1024 run was
selected arbitrarily, however the profiles for the other strong scaling runs
demonstrated a similar trend.

Although initially promsing at scales of over 2000 cores, it is not entirely
clear that the algorithms we have developed are sufficient due to the large
mismatch between IO and everything else.  The profile suggests that IO
accounts for nearly 85\% of program execution.  This is not a stretch of the
imagination since our software merely reads, subsets, and writes.  Even if our
code performed a significant calculation after each read of the edge variable,
the profile would likely remain IO bound but less so.

The reason for the disproportionately faster write bandwidth versus read
bandwith is likely due to caching by the Lustre filesystem.  The relatively
small amount of data being written is likely being copied to an internal
buffer allowing the functions to return rather quickly.  Furthermore, there is
ongoing work to allow the subsetter program to read in a less substantial
amount of data based on the desired subset.  As it is now, the subsetter reads
the entire horizontal domain and immediately culls what falls outside of the
specified region.  For example, the MJO region is roughly 6.5\% of the global
data.  An idea worth further research would be to implement a collective read
routine similar to the one-sided \verb+NGA_Gather+ provided by GA.

%\subsection{Weak Scaling}

%\begin{figure}[!t]
%\center
%\resizebox{3.5in}{!}{
%TODO - FIGURE
%%\input{plots/weak}
%}
%\caption{Weak Scaling Test}
%\label{fig:weak}
%\end{figure}

%\begin{figure}[!t]
%\center
%\resizebox{3.5in}{!}{
%TODO - FIGURE
%%\input{plots/weak_io}
%}
%\caption{Weak Scaling Test - IO}
%\label{fig:weak_io}
%\end{figure}

%\begin{figure*}[!t]
%\center
%\begin{tabular}{lllllll}
%Name&Calls&Prcnt&Times&Prcnt&T/call&Prcnt\\
%TODO&TODO&TODO&TODO&TODO&TODO&TODO\\
%\end{tabular}
%\caption{Weak Scaling Profile for Process 0}
%\label{fig:weak_prof}
%\end{figure*}

%This test was run over 24 timesteps while varying both the number of
%processors as well as the resolution of the model, affecting the amount of
%data processed.  Figure \ref{fig:weak} shows the timing results of the test
%while Figure \ref{fig:weak_io} shows the IO bandwidth.  The number of
%processors as well as the model resolution was doubled for each run starting
%from 16 processors at a model resolution of 125Km through 512 processors at a
%model resolution of 4Km.
