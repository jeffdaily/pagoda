\section{Evaluation}
\label{section:evaluation}

\begin{figure}[!t]
\center
\resizebox{3.5in}{!}{
\input{plots/strong_hist}
}
\caption{Strong Scaling Test.  First, the subsetter was run as a whole
program ("Subsetter").  Lastly, nearly all IO was stripped from the program
to capture the performance of the packing routines ("Algorithms").  At smaller
numbers of cores, there is a great disconnect between time spent in IO and
time spent everywhere else.}
\label{fig:strong}
\end{figure}

\begin{figure}[!t]
\center
\resizebox{3.5in}{!}{
\input{plots/strong_io_hist}
}
\caption{Strong Scaling Test for IO Bandwidth.  The Read and Write bandwidths
are compared as the number of cores are increased.  In general, both
bandwidths increased with respect to the number of cores.}
\label{fig:strong_io}
\end{figure}

\begin{table*}[!t]
\center
\caption{Strong Scaling Profile for Process 0 at 1024 Cores}
\label{tab:strong_prof}
\begin{tabular}{lrrrrrr}
Name&Calls&Percent Calls&Times&Percent Times&T/call\\
NetcdfVariable::read()&39&0.1&47710000&0.79&1223333.3\\
NetcdfFileWriter::write(int,int,int)&34&0.1&8460000&0.14&248823.5\\
ConnectivityVariable::reindex()&3&0.0&1480000&0.02&493333.3\\
VariableDecorator::read()&5&0.0&780000&0.01&156000.0\\
NetcdfDataset::NetcdfDataset(string)&5&0.0&670000&0.01&134000.0\\
Dataset::adjust\_masks(LatLonBox)&1&0.0&600000&0.01&600000.0\\
\end{tabular}
\end{table*}

In order to evaluate the scalability of our subsetter we performed strong
scaling tests.  All tests were performed on the franklin Cray-XT4
supercomputer\cite{franklin} located at NERSC\cite{NERSC}.  The performance
data collected was generated by custom wrappers to the functions we developed.
The IO wrappers perform barriers before and after each collective IO operation
is called with the start and end timestamps collected immediately after each
barrier using \verb=clock_gettime()= or \verb=gettimeofday()= depending on
platform availability.  Before program termination, the number of bytes
written and read and the total time spent performing IO is collected on the
zeroth process and displayed in terms of Gigabytes/second.  The profiling
information collects the number of times each function is called and how much
time was spent in each function.  It is only reported for the zeroth process
and is meant only as a general measure of performance.  The IO and function
profile data were collected on separate runs so that the collection of one
(with additional barriers) would not affect the other.  This test was run over
24 timesteps of an edge variable at a 4 kilometer resolution (R=11) specifying
a subset region of the Madden-Julien Oscillation\cite{MJO}
(20N,-20S,160E,90E).  One timestep of the edge variable is 12.1875 GB.  The
number of processors was doubled each run starting from 64 through 2048.

Fig. \ref{fig:strong} shows the timing results of the test.  Two versions of
the code were run.  "Subsetter" represents the entire program while
"Algorithms" represents executing with nearly all IO turned off.  The latter
case is important in order to evaluate the scalability of our packing
algorithms.  In the latter case, reading of the grid topology was still
required resulting in an insubstantial amount of IO.  Clearly, IO represents a
significant portion of the execution time.  The benefit of additonal
processors appears to have reached a plateau near 2048 processors, however
this may be due to the size of the problem since run times at scale are only
a few minutes.

Fig. \ref{fig:strong_io} shows the IO bandwidth results of the test.  This
test captures the scalability of the IO system which correlates with the
scalability portrayed in Fig. \ref{fig:strong}.  Although the read bandwidth
may have benefited from even more cores, the write bandwidth appears to have
reached a plateau.  Theoretical bandwidth for this system is 16
Gigabytes/second.

Table \ref{tab:strong_prof} is represents a profile for process zero for the
1024 process run.  The function names are sufficiently self-documenting for
those familiar with C++ syntax.  The zeroth process represents a worst-case
scenario since the default distribution of data using GA will always put data
on at least the zeroth process implying process zero should have at least as
much work to perform as any other processor.  The profile for the 1024 run was
selected arbitrarily, however the profiles for the other strong scaling runs
demonstrated a similar trend.  The functions which did not contribute a
significant amount of time to the program were removed and the table is sorted
by time.  The number of calls to the read and write routines were small; not
shown in \ref{tab:strong_prof} is the number of calls to the function
comparing each latitude/longitude pair to the desired subset region.  However,
the amount of time spent in the read and write routines was significant.  In
all cases, the amount of time spent in IO was at least 75\% of the program's
execution.  IO is clearly the dominant factor in running our code.

\subsection{Discussion}

Running our subsetter at a scale of over 2000 cores is initially promising.
Fig. \ref{fig:strong} demonstrates that the algorithms we developed scale
irregardless of the time spent in IO.

The profile suggests that IO accounts for nearly 85\% of program execution.
This is not a stretch of the imagination since our software reads, subsets,
and then writes.  Even if our code performed a significant calculation after
each read of the edge variable, the profile would likely remain IO bound but
less so.

The reason for the disproportionately faster write bandwidth versus read
bandwith seen in Fig. \ref{fig:strong_io} is likely due to caching by the
Lustre filesystem.  The relatively small amount of data being written is
likely being copied to an internal buffer allowing the functions to return
rather quickly.  Furthermore, there is ongoing work to allow the subsetter
program to read in a less substantial amount of data based on the desired
subset.  As it is now, the subsetter reads the entire horizontal domain and
immediately culls what falls outside of the specified region.  For example,
the MJO region is roughly 6.5\% of the global data.
