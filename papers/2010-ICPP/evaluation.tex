\section{Evaluation}
\label{section:evaluation}

\begin{figure}[!t]
\center
\resizebox{3.5in}{!}{
\input{plots/strong_hist}
}
\caption{Strong Scaling Test.  First, the subsetter was run as a whole
program ("Subsetter").  Lastly, nearly all IO was stripped from the program
to capture the performance of the packing routines ("Algorithms").  At smaller
numbers of cores, there is a great disconnect between time spent in IO and
time spent everywhere else.}
\label{fig:strong}
\end{figure}

\begin{figure}[!t]
\center
\resizebox{3.5in}{!}{
\input{plots/strong_io_hist}
}
\caption{Strong Scaling Test for IO Bandwidth.  The Read and Write bandwidths
are compared as the number of cores are increased.  In general, both
bandwidths increased with respect to the number of cores.}
\label{fig:strong_io}
\end{figure}

\begin{table*}[!t]
\center
\caption{Profile for Process 0 at 2048 Cores}
\label{tab:strong_prof}
\begin{tabular}{lrrrrrr}
Name&Calls&Percent Calls&Time (seconds)&Percent Time&Time/call (seconds)\\
NetcdfVariable::read()                    &39&0.1&156.1&93.0&4.00\\
NetcdfFileWriter::write(int,int,int)      &34&0.1&  5.9& 3.5&0.17\\
VariableDecorator::read()                 & 5&0.0&  1.7& 1.0&0.34\\
NetcdfDataset::NetcdfDataset(string)      & 5&0.0&  1.2& 0.7&0.24\\
Dataset::adjust\_masks(LatLonBox)         & 1&0.0&  1.0& 0.6&1.05\\
ConnectivityVariable::reindex()           & 3&0.0&  0.7& 0.4&0.25\\
NetcdfFileWriter::NetcdfFileWriter(string)& 1&0.0&  0.2& 0.2&0.29\\
\end{tabular}
\end{table*}

In order to evaluate the scalability of our subsetter we performed strong
scaling tests.  All tests were performed on the Franklin Cray-XT4
supercomputer\cite{franklin} located at NERSC\cite{NERSC}.  The Franklin
supercomputer features 9,572 Opteron 2.3 gigahertz quad core processors with 2
gigabytes of memory per core.  The Lustre parallel file system supports a
theoretical peak of 16 gigabytes/second to each of two /scratch file systems.
The performance data collected was generated by custom wrappers to the
functions we developed.  The IO wrappers perform barriers before and after
each collective IO operation is called with the start and end timestamps
collected immediately after each barrier using \verb=clock_gettime()= or
\verb=gettimeofday()= depending on platform availability.  Before program
termination, the number of bytes written and read and the total time spent
performing IO is collected on the zeroth process and displayed in terms of
Gigabytes/second.  The profiling information collects the number of times each
function is called and how much time was spent in each function.  It is only
reported for the zeroth process and is meant only as a general measure of
performance.  The IO and function profile data were collected on separate runs
so that the collection of one (with additional barriers) would not affect the
other.  This test was run over 24 timesteps of an edge variable at a 4
kilometer resolution (R=11) specifying a subset region of the Madden-Julien
Oscillation\cite{MJO} (20N,-20S,160E,90E).  One timestep of the edge variable
is 12.1875 GB.  The number of processors was doubled each run starting from 64
through 2048.

Fig. \ref{fig:strong} shows the timing results of the test.  Two versions of
the code were run.  "Subsetter" represents the entire program while
"Algorithms" represents executing with nearly all IO turned off.  The latter
case is important in order to evaluate the scalability of our packing
algorithms.  Since our current algorithms do not involve any algebraic
operations, the "Algorithms" case is an accurate representation of
communication only.  In the "Algorithms" case, reading of the grid topology was
still required resulting in an insubstantial amount of IO.  Clearly, IO
represents a significant portion of the execution time.  The benefit of
additonal processors appears to have reached a plateau near 1024 processors for
both the "Subsetter" and "Algorithms" cases, however this may be due to the
size of the problem since run times at this scale are only a few minutes in
length.

Fig. \ref{fig:strong_io} shows the IO bandwidth results of the test.  This test
captures the scalability of the IO system which correlates with the scalability
portrayed in Fig. \ref{fig:strong}.  Although the read bandwidth may have
benefited from additional cores beyond 2048, the write bandwidth appears to
have reached a plateau.  The hardware is likely reaching its saturation point
if not already there.

Table \ref{tab:strong_prof} is represents a profile for process zero for the
1024 process run.  The function names are sufficiently self-documenting for
those familiar with C++ syntax.  The zeroth process represents a worst-case
scenario since the default distribution of data using GA will always put data
on at least the zeroth process implying process zero should have at least as
much work to perform as any other processor.  The profile for the 1024 run was
selected arbitrarily, however the profiles for the other strong scaling runs
demonstrated a similar trend.  The functions which did not contribute a
significant amount of time to the program were removed and the table is sorted
by time.  The number of calls to the read and write routines were small; not
shown in \ref{tab:strong_prof} is the number of calls to the function
comparing each latitude/longitude pair to the desired subset region.  However,
the amount of time spent in the read and write routines was significant.  In
all cases, the amount of time spent in IO was at least 75\% of the program's
execution.  IO is clearly the dominant factor in running our code.

\subsection{Discussion}

Running our subsetter at a scale of over 2000 cores is initially promising.
Fig. \ref{fig:strong} demonstrates that the algorithms we developed scale
regardless of the time spent in IO.

The profile suggests that IO accounts for nearly 95\% of program execution at
2048 cores.  This is not a stretch of the imagination since our software reads,
subsets, and then writes.  Even if our code performed a significant calculation
after each read of the edge variable, the profile would likely remain IO bound
but less so.  At fewer numbers of cores the percentage of time spent in IO
ranged from 75\% to the 95\% shown in Table \ref{tab:strong_prof}.

The reason for the disproportionately faster write bandwidth versus read
bandwith seen in Fig. \ref{fig:strong_io} is likely due to caching by the
Lustre parallel filesystem.  The relatively small amount of data being written
is likely being copied to an internal buffer allowing the functions to return
rather quickly.  There is ongoing work to allow the subsetter program to read
in a less substantial amount of data based on the desired subset.  As it is
now, the subsetter reads the entire horizontal domain and immediately culls
what falls outside of the specified region.  For example, the MJO region is
roughly 6.5\% of the global data so over 90\% of the data that is read in is
unused.
